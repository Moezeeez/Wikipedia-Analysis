# Wikipedia Book Data Extraction and Analysis

This project involves downloading and processing 50GB of compressed Wikipedia data to extract and analyze book-related articles. The process included data extraction, parsing, and organizing relevant book information from the Wikipedia dump. A total of 40 hours of runtime was utilized to handle the data operations. This repository provides scripts and insights gained from the analysis of this large-scale dataset.
